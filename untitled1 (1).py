# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TTbnqzPMAjAtbkghdDd2gulOO3bMxzYg
"""

# Install dependencies
!pip install gradio transformers torch pdfplumber python-docx spacy

# Download spaCy's language model (if not already installed)
!python -m spacy download en_core_web_sm

# Now you can proceed with your chatbot code
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import spacy
import pdfplumber
from docx import Document

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.3-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.3-2b-instruct")

# Load spaCy for Named Entity Recognition (NER)
nlp = spacy.load("en_core_web_sm")

# Function to process and simplify clauses
def simplify_clause(text):
    messages = [{"role": "user", "content": f"Rewrite the following legal clause in simple terms: {text}"}]
    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=100)
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])
    return response

# Function for Named Entity Recognition (NER)
def extract_entities(text):
    doc = nlp(text)
    entities = [{"text": ent.text, "label": ent.label_} for ent in doc.ents]
    return entities

# Function to classify document type (stub for simplicity)
def classify_document(text):
    if "non-disclosure" in text.lower():
        return "NDA (Non-Disclosure Agreement)"
    elif "lease" in text.lower():
        return "Lease Agreement"
    elif "employment" in text.lower():
        return "Employment Contract"
    else:
        return "Unknown Document Type"

# Function to extract clauses from a legal document (basic example)
def extract_clauses(text):
    # Simple heuristic: split by paragraphs or specific keywords
    clauses = text.split("\n\n")
    return clauses

# Function to handle document uploads (PDF, DOCX, TXT)
def process_file(file):
    # Extract text from PDF
    if file.name.endswith(".pdf"):
        with pdfplumber.open(file.name) as pdf:
            text = "\n".join([page.extract_text() for page in pdf.pages])
    # Extract text from DOCX
    elif file.name.endswith(".docx"):
        doc = Document(file.name)
        text = "\n".join([para.text for para in doc.paragraphs])
    # Extract text from TXT
    elif file.name.endswith(".txt"):
        with open(file.name, "r") as txt_file:
            text = txt_file.read()
    else:
        return "Unsupported file format."

    return text

# Main function to process user inputs
def legal_document_analyzer(user_input, file=None):
    if file:
        document_text = process_file(file)
        return {
            "Simplified Clauses": simplify_clause(document_text),
            "Named Entities": extract_entities(document_text),
            "Clauses Extracted": extract_clauses(document_text),
            "Document Type": classify_document(document_text)
        }
    else:
        # User input is for specific clause simplification or NER
        simplified_clause = simplify_clause(user_input)
        named_entities = extract_entities(user_input)
        return {
            "Simplified Clause": simplified_clause,
            "Named Entities": named_entities
        }

# Create the Gradio interface
iface = gr.Interface(
    fn=legal_document_analyzer,
    inputs=[gr.Textbox(label="Enter Clause or Text"), gr.File(label="Upload Document (PDF, DOCX, TXT)")],
    outputs=["json"],
    title="AI-Powered Legal Document Analyzer",
    description="Simplify, decode, and classify legal documents with AI."
)

# Launch the interface
iface.launch()